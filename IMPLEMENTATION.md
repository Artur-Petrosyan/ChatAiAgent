# Документация по реализации Ollama LangGraph Agent

## Обзор проекта

Создан простой AI-агент на базе LangGraph.js, который использует локальную LLM через Ollama для ответов на вопросы пользователя. Проект включает веб-интерфейс на React + Vite и Express API сервер.

## Архитектура

### Компоненты системы

1. **LangGraph Agent** (`src/agent/`)
   - Определение состояния графа
   - Узлы для обработки сообщений
   - Граф выполнения агента

2. **Express API Server** (`src/server/`)
   - REST API endpoint для обработки запросов
   - Интеграция с LangGraph агентом

3. **React Frontend** (`src/`)
   - UI для взаимодействия с агентом
   - Отправка сообщений и отображение ответов

## Детали реализации

### 1. Определение состояния (state.ts)

Использован Annotation API из LangGraph для определения состояния:

```typescript
export const AgentState = Annotation.Root({
  ...MessagesAnnotation.spec,  // Встроенная аннотация для сообщений
  llmCalls: Annotation<number>({
    reducer: (x, y) => x + y,   // Reducer для суммирования вызовов
    default: () => 0,
  }),
});
```

**Что было сделано:**
- Использован `MessagesAnnotation` для работы с сообщениями
- Добавлено поле `llmCalls` для отслеживания количества вызовов LLM
- Определен тип состояния для TypeScript

### 2. Узлы графа (nodes.ts)

Создан узел для вызова LLM через Ollama:

```typescript
const model = new ChatOllama({
  baseUrl: "http://localhost:11434",
  model: "llama3.2",
  temperature: 0.7,
});
```

**Что было сделано:**
- Интегрирован `ChatOllama` из `@langchain/community`
- Настроен системный промпт для русского языка
- Узел принимает состояние и возвращает обновленное состояние с ответом LLM

### 3. Граф выполнения (graph.ts)

Построен простой граф с одним узлом LLM:

```typescript
export const agentGraph = new StateGraph(AgentState)
  .addNode("llm", llmNode)
  .addEdge(START, "llm")
  .addConditionalEdges("llm", shouldContinue, {
    llm: "llm",
    [END]: END,
  })
  .compile();
```

**Что было сделано:**
- Использован Graph API для явного определения графа
- Добавлен условный переход для завершения выполнения
- Граф компилируется один раз при загрузке модуля

**Логика завершения:**
- Если последнее сообщение от AI - завершаем (END)
- Иначе продолжаем с вызовом LLM (но в текущей реализации это не используется, так как граф простой)

### 4. Express API Server (server/index.ts)

Создан REST API для взаимодействия с агентом:

**Endpoints:**
- `POST /api/chat` - обработка сообщений пользователя
- `GET /api/health` - проверка статуса сервера

**Что было сделано:**
- Настроен CORS для работы с фронтендом
- Обработка ошибок с возвратом понятных сообщений
- Преобразование входящих сообщений в `HumanMessage`
- Извлечение ответа из результата выполнения графа

### 5. React Frontend (App.tsx)

Создан простой чат-интерфейс:

**Функциональность:**
- Отображение истории сообщений
- Отправка сообщений через API
- Индикатор загрузки
- Обработка ошибок

**Что было сделано:**
- Использован React hooks (useState)
- Асинхронная отправка запросов к API
- Красивый UI с градиентами и анимациями
- Адаптивный дизайн

### 6. Конфигурация проекта

**package.json:**
- Установлены необходимые зависимости:
  - `@langchain/langgraph` - основной фреймворк
  - `@langchain/community` - интеграция с Ollama
  - `@langchain/core` - базовые типы и сообщения
  - `express` - backend сервер
  - `react`, `vite` - фронтенд

**vite.config.ts:**
- Настроен прокси для API запросов
- Порт 3000 для фронтенда
- Проксирование `/api` на `localhost:3001`

**tsconfig.json:**
- Настроен TypeScript для современного JavaScript
- Поддержка React JSX
- Строгий режим проверки типов

## Особенности реализации

### Использование Annotation API

Вместо традиционного Zod подхода использован Annotation API, как рекомендовано в документации:

**Преимущества:**
- Более современный подход
- Встроенная поддержка reducer функций
- Проще работать с сообщениями через `MessagesAnnotation`

### Простая архитектура графа

Выбрана простая архитектура с одним узлом LLM, так как:
- Агент не требует инструментов (tools)
- Нет сложной условной логики
- Основная задача - ответ на вопрос пользователя

### Интеграция с Ollama

Использован `ChatOllama` из `@langchain/community`:
- Поддержка локальных моделей
- Простая настройка через baseUrl
- Совместимость с LangChain сообщениями

## Запуск и использование

### Команды разработки

```bash
# Установка зависимостей
npm install

# Запуск в режиме разработки (одновременно сервер и клиент)
npm run dev

# Только сервер
npm run dev:server

# Только клиент
npm run dev:client
```

### Проверка работы

1. Убедитесь, что Ollama запущен:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. Откройте http://localhost:3000 в браузере

3. Отправьте сообщение агенту

## Возможные улучшения

1. **Добавление инструментов (tools)**
   - Можно расширить агента инструментами для выполнения действий
   - Использовать паттерн ReAct из документации

2. **Память между сессиями**
   - Добавить `MemorySaver` для сохранения состояния
   - Использовать `InMemoryStore` для долгосрочной памяти

3. **Потоковая передача**
   - Реализовать streaming ответов от LLM
   - Отображать токены в реальном времени

4. **Обработка ошибок**
   - Более детальная обработка ошибок Ollama
   - Retry логика для неудачных запросов

5. **Визуализация графа**
   - Добавить отображение структуры графа
   - Показывать текущее состояние выполнения

## Заключение

Реализован рабочий AI-агент на базе LangGraph.js с интеграцией Ollama. Проект следует лучшим практикам из документации и использует современные подходы (Annotation API). Код структурирован, типизирован и готов к расширению.

